{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run this notebook properly you first need to install all the libraries used\n",
    "## To install the libraries use \"pip install <library name> command\"\n",
    "\n",
    "The libraries used are :\n",
    "- Pandas\n",
    "- nltk\n",
    "    \n",
    "## Also make sure the path to the files training_data.tsv and eval_data.txt are changed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The approach I have used here is Chunking which is a part of NLP and is done through NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/harshpanwar/Desktop/phrase_extractor/training_data.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view the first 5 rows in the dataframe created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Make remainder</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Set a reminder on date 23rd November'2016</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I need a daily wake up call</td>\n",
       "      <td>wake up call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>remind me 6 pm today eveng</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Hi Pls to make one reminder for me</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sent         label\n",
       "0                             Make remainder     Not Found\n",
       "1  Set a reminder on date 23rd November'2016     Not Found\n",
       "2                I need a daily wake up call  wake up call\n",
       "3                 remind me 6 pm today eveng     Not Found\n",
       "4         Hi Pls to make one reminder for me     Not Found"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To remove those rows which don't have a reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.label != 'Not Found']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5907"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I need a daily wake up call</td>\n",
       "      <td>wake up call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Remind me at 28 December for recharge</td>\n",
       "      <td>recharge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Can u pls remind me at 7pm on 8 Jan</td>\n",
       "      <td>on 8 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>What is my next reminder?</td>\n",
       "      <td>What</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Set a reminder on 4 th Dec of going to meet so...</td>\n",
       "      <td>meet sonal miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent            label\n",
       "0                        I need a daily wake up call     wake up call\n",
       "1              Remind me at 28 December for recharge         recharge\n",
       "2                Can u pls remind me at 7pm on 8 Jan         on 8 Jan\n",
       "3                          What is my next reminder?             What\n",
       "4  Set a reminder on 4 th Dec of going to meet so...  meet sonal miss"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df.sent[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remind me to buy eggs on next Monday and Tuesday at 9pm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Remind',\n",
       " 'me',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'eggs',\n",
       " 'on',\n",
       " 'next',\n",
       " 'Monday',\n",
       " 'and',\n",
       " 'Tuesday',\n",
       " 'at',\n",
       " '9pm']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Remind', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('buy', 'VB'),\n",
       " ('eggs', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('next', 'JJ'),\n",
       " ('Monday', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Tuesday', 'NNP'),\n",
       " ('at', 'IN'),\n",
       " ('9pm', 'CD')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Remind', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('buy', 'VB'),\n",
       " ('eggs', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('next', 'JJ'),\n",
       " ('Monday', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Tuesday', 'NNP'),\n",
       " ('at', 'IN'),\n",
       " ('9pm', 'CD')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = chunkParser.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Remind/VB\n",
      "  me/PRP\n",
      "  to/TO\n",
      "  buy/VB\n",
      "  eggs/NNS\n",
      "  on/IN\n",
      "  next/JJ\n",
      "  Monday/NNP\n",
      "  and/CC\n",
      "  Tuesday/NNP\n",
      "  at/IN\n",
      "  9pm/CD)\n"
     ]
    }
   ],
   "source": [
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To better visualize the process of chunking run the code \"tree.draw()\" below which will draw a tree for having different POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Remind', 'buy', 'eggs', 'Monday', 'Tuesday']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Significant words\n",
    "sig_words = [s[0] for s in tagged if s[1].startswith('N') or s[1].startswith('V')]\n",
    "sig_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Remind',), ('me',), ('to',), ('buy',), ('eggs',), ('on',), ('next',), ('Monday',), ('and',), ('Tuesday',), ('at',), ('9pm',)] \n",
      "\n",
      "[('Remind', 'me'), ('me', 'to'), ('to', 'buy'), ('buy', 'eggs'), ('eggs', 'on'), ('on', 'next'), ('next', 'Monday'), ('Monday', 'and'), ('and', 'Tuesday'), ('Tuesday', 'at'), ('at', '9pm')] \n",
      "\n",
      "[('Remind', 'me', 'to'), ('me', 'to', 'buy'), ('to', 'buy', 'eggs'), ('buy', 'eggs', 'on'), ('eggs', 'on', 'next'), ('on', 'next', 'Monday'), ('next', 'Monday', 'and'), ('Monday', 'and', 'Tuesday'), ('and', 'Tuesday', 'at'), ('Tuesday', 'at', '9pm')] \n",
      "\n",
      "[('Remind', 'me', 'to', 'buy'), ('me', 'to', 'buy', 'eggs'), ('to', 'buy', 'eggs', 'on'), ('buy', 'eggs', 'on', 'next'), ('eggs', 'on', 'next', 'Monday'), ('on', 'next', 'Monday', 'and'), ('next', 'Monday', 'and', 'Tuesday'), ('Monday', 'and', 'Tuesday', 'at'), ('and', 'Tuesday', 'at', '9pm')] \n",
      "\n",
      "[('Remind', 'me', 'to', 'buy', 'eggs'), ('me', 'to', 'buy', 'eggs', 'on'), ('to', 'buy', 'eggs', 'on', 'next'), ('buy', 'eggs', 'on', 'next', 'Monday'), ('eggs', 'on', 'next', 'Monday', 'and'), ('on', 'next', 'Monday', 'and', 'Tuesday'), ('next', 'Monday', 'and', 'Tuesday', 'at'), ('Monday', 'and', 'Tuesday', 'at', '9pm')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    out = list(ngrams(token, i))\n",
    "    print(out,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_extractor(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    nltk.pos_tag(words)\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    parser = nltk.RegexpParser(grammar)\n",
    "    t = parser.parse(nltk.pos_tag(words))\n",
    "    a = [s for s in t.subtrees() if s.label() == \"NP\"]\n",
    "    c = []\n",
    "    num = []\n",
    "    key  = [\"monday\",\"tuesday\", \"wednesday\", \"thursday\",\"friday\",\"saturday\",\"sunday\",\"today\",\"tomorrow\",\"yesterday\", \"reminder\", \"remind\", \"th\", \"pm\",\"am\"]\n",
    "    for i in range(len(a)):\n",
    "        count=0\n",
    "        phrase = \"\"\n",
    "        for j in range(len(a[i])):\n",
    "            if a[i][j][0].lower() in key:\n",
    "                phrase = phrase\n",
    "            else : \n",
    "                phrase = phrase + str(a[i][j][0]) + \" \"\n",
    "                count = count+1\n",
    "        c.append(phrase)\n",
    "        num.append(count)\n",
    "    if(c==[] or max(num)<=1):\n",
    "        return \"Not Found\"\n",
    "    else :\n",
    "        maxi = max(num)\n",
    "        for i in range(len(num)):\n",
    "            if(num[i]==maxi):\n",
    "                return c[i].rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use phase_extractor method created above to extract a phrase/reminder from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remind me to buy eggs on next Monday and Tuesday at 9pm \n",
      "\n",
      "Reminder extracted  :    Not Found\n"
     ]
    }
   ],
   "source": [
    "print(sentence,\"\\n\")\n",
    "print(\"Reminder extracted  :   \", phrase_extractor(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We were able to extract a reminder as seen above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply the same method on a single sentence chosen randomly from the eval_data.txt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminder 11am tomorrorw buy bell for papu\n",
      "Reminder extracted  :    tomorrorw buy\n"
     ]
    }
   ],
   "source": [
    "with open(\"eval_data.txt\", 'r+') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "print(lines[5])\n",
    "print(\"Reminder extracted  :   \",phrase_extractor(lines[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We apply the function phrase_extractor on all the lines in the eval_data.txt dataset and store the output in the form of csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('eval_data.csv', mode='w', newline='') as csv_file:\n",
    "    fields = ['sent', 'label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(len(lines)):\n",
    "        writer.writerow({'sent':lines[i],'label':phrase_extractor(lines[i])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To evaluate our trained model we create a new csv file \"eval_data_accuracy.csv\" containing three fields 'sent' , 'Actual_label' , 'Predicted_label' . The sent label is same as the sent label of the initial dataframe df which contains the sentences. The Actual_label have the predicted values of reminders that were already stored in the training dataset. And the Predicted_label contains the Reminders that were generated by applying phrase_extractor function on the sent label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  1.946842728965634 %\n"
     ]
    }
   ],
   "source": [
    "with open('eval_data_accuracy.csv', mode='w', newline='', encoding = 'utf-8') as csv_file:\n",
    "    fields = ['sent', 'Actual_label', 'Predicted_label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    count = 0\n",
    "    for i in range(len(df)):\n",
    "        writer.writerow({'sent':df['sent'][i], 'Actual_label':df['label'][i], 'Predicted_label':phrase_extractor(str(df['sent'][i]))})\n",
    "        \n",
    "        if str(df['label'][i]) == phrase_extractor(str(df['sent'][i])):\n",
    "            count = count+1\n",
    "            \n",
    "print (\"Accuracy = \", (count/len(df))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Accuracy can be increased by changing the grammar which we considered above. \n",
    "- The accuracy depends on the training dataset. As we have just considered only those predictions as true which are exactly similar to the values in the training_data.tsv under \"label\" then it may be possible that some predictions made by our model are correct but the training_data.tsv have the wrong reminder. \n",
    "- By removing the rows having the label Not Found we have decreased the size of the dataset by around 40%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
