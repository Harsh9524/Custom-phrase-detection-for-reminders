{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run this notebook properly you first need to install all the libraries used\n",
    "## To install the libraries use \"pip install <library name> command\"\n",
    "\n",
    "The libraries used are :\n",
    "- Pandas\n",
    "- nltk\n",
    "    \n",
    "## Also make sure the path to the files training_data.tsv and eval_data.txt are changed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The approach I have used here is Chunking which is a part of NLP and is done through NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/harshpanwar/Desktop/training_data.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view the first 5 rows in the dataframe created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Make remainder</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Set a reminder on date 23rd November'2016</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I need a daily wake up call</td>\n",
       "      <td>wake up call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>remind me 6 pm today eveng</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Hi Pls to make one reminder for me</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sent         label\n",
       "0                             Make remainder     Not Found\n",
       "1  Set a reminder on date 23rd November'2016     Not Found\n",
       "2                I need a daily wake up call  wake up call\n",
       "3                 remind me 6 pm today eveng     Not Found\n",
       "4         Hi Pls to make one reminder for me     Not Found"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9819"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To remove those rows which don't have a reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.label != 'Not Found']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5907"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I need a daily wake up call</td>\n",
       "      <td>wake up call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Remind me at 28 December for recharge</td>\n",
       "      <td>recharge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Can u pls remind me at 7pm on 8 Jan</td>\n",
       "      <td>on 8 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>What is my next reminder?</td>\n",
       "      <td>What</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Set a reminder on 4 th Dec of going to meet so...</td>\n",
       "      <td>meet sonal miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent            label\n",
       "0                        I need a daily wake up call     wake up call\n",
       "1              Remind me at 28 December for recharge         recharge\n",
       "2                Can u pls remind me at 7pm on 8 Jan         on 8 Jan\n",
       "3                          What is my next reminder?             What\n",
       "4  Set a reminder on 4 th Dec of going to meet so...  meet sonal miss"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df.sent[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set a reminder on 4 th Dec of going to meet sonal miss at 2:00 pm'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Set',\n",
       " 'a',\n",
       " 'reminder',\n",
       " 'on',\n",
       " '4',\n",
       " 'th',\n",
       " 'Dec',\n",
       " 'of',\n",
       " 'going',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'sonal',\n",
       " 'miss',\n",
       " 'at',\n",
       " '2:00',\n",
       " 'pm']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Set', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('reminder', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('4', 'CD'),\n",
       " ('th', 'JJ'),\n",
       " ('Dec', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('meet', 'VB'),\n",
       " ('sonal', 'JJ'),\n",
       " ('miss', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('2:00', 'CD'),\n",
       " ('pm', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Set', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('reminder', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('4', 'CD'),\n",
       " ('th', 'JJ'),\n",
       " ('Dec', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('meet', 'VB'),\n",
       " ('sonal', 'JJ'),\n",
       " ('miss', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('2:00', 'CD'),\n",
       " ('pm', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = chunkParser.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Set/VB\n",
      "  (NP a/DT reminder/NN)\n",
      "  on/IN\n",
      "  4/CD\n",
      "  th/JJ\n",
      "  Dec/NNP\n",
      "  of/IN\n",
      "  going/VBG\n",
      "  to/TO\n",
      "  meet/VB\n",
      "  sonal/JJ\n",
      "  miss/NNS\n",
      "  at/IN\n",
      "  2:00/CD\n",
      "  (NP pm/NN))\n",
      "(NP a/DT reminder/NN)\n",
      "(NP pm/NN)\n"
     ]
    }
   ],
   "source": [
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To better visualize the process of chunking run the code \"tree.draw()\" below which will draw a tree for having different POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Set', 'reminder', 'Dec', 'going', 'meet', 'miss', 'pm']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Significant words\n",
    "sig_words = [s[0] for s in tagged if s[1].startswith('N') or s[1].startswith('V')]\n",
    "sig_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Set',), ('a',), ('reminder',), ('on',), ('4',), ('th',), ('Dec',), ('of',), ('going',), ('to',), ('meet',), ('sonal',), ('miss',), ('at',), ('2:00',), ('pm',)] \n",
      "\n",
      "[('Set', 'a'), ('a', 'reminder'), ('reminder', 'on'), ('on', '4'), ('4', 'th'), ('th', 'Dec'), ('Dec', 'of'), ('of', 'going'), ('going', 'to'), ('to', 'meet'), ('meet', 'sonal'), ('sonal', 'miss'), ('miss', 'at'), ('at', '2:00'), ('2:00', 'pm')] \n",
      "\n",
      "[('Set', 'a', 'reminder'), ('a', 'reminder', 'on'), ('reminder', 'on', '4'), ('on', '4', 'th'), ('4', 'th', 'Dec'), ('th', 'Dec', 'of'), ('Dec', 'of', 'going'), ('of', 'going', 'to'), ('going', 'to', 'meet'), ('to', 'meet', 'sonal'), ('meet', 'sonal', 'miss'), ('sonal', 'miss', 'at'), ('miss', 'at', '2:00'), ('at', '2:00', 'pm')] \n",
      "\n",
      "[('Set', 'a', 'reminder', 'on'), ('a', 'reminder', 'on', '4'), ('reminder', 'on', '4', 'th'), ('on', '4', 'th', 'Dec'), ('4', 'th', 'Dec', 'of'), ('th', 'Dec', 'of', 'going'), ('Dec', 'of', 'going', 'to'), ('of', 'going', 'to', 'meet'), ('going', 'to', 'meet', 'sonal'), ('to', 'meet', 'sonal', 'miss'), ('meet', 'sonal', 'miss', 'at'), ('sonal', 'miss', 'at', '2:00'), ('miss', 'at', '2:00', 'pm')] \n",
      "\n",
      "[('Set', 'a', 'reminder', 'on', '4'), ('a', 'reminder', 'on', '4', 'th'), ('reminder', 'on', '4', 'th', 'Dec'), ('on', '4', 'th', 'Dec', 'of'), ('4', 'th', 'Dec', 'of', 'going'), ('th', 'Dec', 'of', 'going', 'to'), ('Dec', 'of', 'going', 'to', 'meet'), ('of', 'going', 'to', 'meet', 'sonal'), ('going', 'to', 'meet', 'sonal', 'miss'), ('to', 'meet', 'sonal', 'miss', 'at'), ('meet', 'sonal', 'miss', 'at', '2:00'), ('sonal', 'miss', 'at', '2:00', 'pm')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for i in range(1,n+1):\n",
    "    out = list(ngrams(token, i))\n",
    "    print(out,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_extractor(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    nltk.pos_tag(words)\n",
    "    grammar = \"NP: {<VB.*>?<RB>?<PRP.*>?<IN>?<DT>?<JJ.*>*<NN.*>+}\"\n",
    "    parser = nltk.RegexpParser(grammar)\n",
    "    t = parser.parse(nltk.pos_tag(words))\n",
    "    a = [s for s in t.subtrees() if s.label() == \"NP\"]\n",
    "    c = []\n",
    "    num = []\n",
    "    key  = [\"monday\",\"tuesday\", \"wednesday\", \"thursday\",\"friday\",\"saturday\",\"sunday\",\"today\",\"tomorrow\",\"yesterday\", \"reminder\", \"remind\", \"th\", \"pm\",\"am\"]\n",
    "    for i in range(len(a)):\n",
    "        count=0\n",
    "        phrase = \"\"\n",
    "        for j in range(len(a[i])):\n",
    "            if a[i][j][0].lower() in key:\n",
    "                phrase = phrase\n",
    "            else : \n",
    "                phrase = phrase + str(a[i][j][0]) + \" \"\n",
    "                count = count+1\n",
    "        c.append(phrase)\n",
    "        num.append(count)\n",
    "    if(c==[] or max(num)<=1):\n",
    "        return \"Not Found\"\n",
    "    else :\n",
    "        maxi = max(num)\n",
    "        for i in range(len(num)):\n",
    "            if(num[i]==maxi):\n",
    "                return c[i].rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use phase_extractor method created above to extract a phrase/reminder from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set a reminder on 4 th Dec of going to meet sonal miss at 2:00 pm \n",
      "\n",
      "Reminder extracted  :    meet sonal miss\n"
     ]
    }
   ],
   "source": [
    "print(sentence,\"\\n\")\n",
    "print(\"Reminder extracted  :   \", phrase_extractor(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We were able to extract a reminder as seen above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply the same method on a single sentence chosen randomly from the eval_data.txt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminder 11am tomorrorw buy bell for papu\n",
      "Reminder extracted  :    tomorrorw buy bell\n"
     ]
    }
   ],
   "source": [
    "with open(\"eval_data.txt\", 'r+') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "print(lines[5])\n",
    "print(\"Reminder extracted  :   \",phrase_extractor(lines[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We apply the function phrase_extractor on all the lines in the eval_data.txt dataset and store the output in the form of csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('eval_data.csv', mode='w', newline='') as csv_file:\n",
    "    fields = ['sent', 'label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(len(lines)):\n",
    "        writer.writerow({'sent':lines[i],'label':phrase_extractor(lines[i])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To evaluate our trained model we create a new csv file \"eval_data_accuracy.csv\" containing three fields 'sent' , 'Actual_label' , 'Predicted_label' . The sent label is same as the sent label of the initial dataframe df which contains the sentences. The Actual_label have the predicted values of reminders that were already stored in the training dataset. And the Predicted_label contains the Reminders that were generated by applying phrase_extractor function on the sent label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  25.342813610970033 %\n"
     ]
    }
   ],
   "source": [
    "with open('eval_data_accuracy.csv', mode='w', newline='', encoding = 'utf-8') as csv_file:\n",
    "    fields = ['sent', 'Actual_label', 'Predicted_label']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    count = 0\n",
    "    for i in range(len(df)):\n",
    "        writer.writerow({'sent':df['sent'][i], 'Actual_label':df['label'][i], 'Predicted_label':phrase_extractor(str(df['sent'][i]))})\n",
    "        \n",
    "        if str(df['label'][i]) == phrase_extractor(str(df['sent'][i])):\n",
    "            count = count+1\n",
    "            \n",
    "print (\"Accuracy = \", (count/len(df))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Accuracy can be increased by changing the grammar which we considered above. \n",
    "- The accuracy depends on the training dataset. As we have just considered only those predictions as true which are exactly similar to the values in the training_data.tsv under \"label\" then it may be possible that some predictions made by our model are correct but the training_data.tsv have the wrong reminder. \n",
    "- By removing the rows having the label Not Found we have decreased the size of the dataset by around 40%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
